<!DOCTYPE html>
<html>

<head>
    <title>PCA</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.1/css/all.css" integrity="sha384-xxzQGERXS00kBmZW/6qxqJPyxW3UR0BPsL4c8ILaIWXva5kFi7TxkIIaMiKtqV1Q" crossorigin="anonymous">
</head>

<body>
    <main>
        <main>
            <nav role="navigation">
                <div id="menuToggle">
                    <!--
                                            A fake / hidden checkbox is used as click reciever,
                                            so you can use the :checked selector on it.
                                            -->
                    <input type="checkbox" />

                    <!--
                                            Some spans to act as a hamburger.
                                            
                                            They are acting like a real hamburger,
                                            not that McDonalds stuff.
                                            -->
                    <span></span>
                    <span></span>
                    <span></span>

                    <!--
                                            Too bad the menu has to be inside of the button
                                            but hey, it's pure CSS magic.
                                            -->
                    <ul id="menu">
                        <a href="welcome.html">
                            <li>Welcome</li>
                        </a>
                        <a href="whatismachinelearning.html">
                            <li>What is Machine Learning?</li>
                        </a>
                        <a href="taskdrivenvsdatadriven.html">
                            <li>Task Driven vs Data Driven</li>
                        </a>
                        <a href="supervisedvsunsupervised.html">
                            <li>Supervised and Unsupervised Learning</li>
                        </a>
                        <a href="classification.html">
                            <li>Classification</li>
                        </a>
                        <ul>
                            <a href="svm.html">
                                <li>Support Vector Machines (SVM)</li>
                            </a>
                            <a href="knearestneighbour.html">
                                <li>Nearest Neighbour</li>
                            </a>
                            <a href="neuralnetworks.html">
                                <li>Neural Networks</li>
                            </a>
                        </ul>
                        <a href="regression.html">
                            <li>Regression</li>
                        </a>
                        <ul>
                            <a href="linearregression.html">
                                <li>Linear Regression</li>
                            </a>
                            <a href="logisticregression.html">
                                <li>Logistic Regression</li>
                            </a>
                        </ul>


                        <a href="clustering.html">
                            <li>Clustering</li>
                        </a>
                        <ul>
                            <a href="kmeans.html">
                                <li>K-Means</li>
                            </a>
                            <a href="gmm.html">
                                <li>Gaussian Mixture Model (GMM)</li>
                            </a>
                        </ul>
                        <a href="dr.html">
                            <li>Dimensionality Reduction</li>
                        </a>
                        <ul>
                            <a href="pca.html">
                                <li>PCA</li>
                            </a>
                            <a href="lda.html">
                                <li>LDA</li>
                            </a>
                        </ul>
                        <a href="associationrule.html">
                            <li>Association Rule</li>
                        </a>
                    </ul>
                </div>
            </nav>

            <div class="container">

                <div class="textBackground">
                    <img class="titlePageImg" src="./Images/PCA title.png" alt="Neural Network Title Image" />
                </div>

                <div class="textBackgroundLight">
                    <p>
                        Principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD. It uses
                        the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract. It can also use the scipy.sparse.linalg
                        ARPACK implementation of the truncated SVD.
                    </p>
                </div>

                <div class="textBackground">
                    <h2>What is a PCA?</h2>
                </div>
                <div class="textBackgroundLight">
                    <p>
                        PCA is a linear method used to reduce data dimensionality. PCA will aim to reduce the dimensionality, removing the interrelated variables, while retaining as much as possible. Keeping as much variation possible that is present in the data set.
                    </p>
                    <p>
                        The dimensionality reduction is achieved by transforming the data to a new set of variables, these are called the principal components (PCs). These PCs are uncorrelated, and are ordered in a way that the first couple of PCs retains the most amount of
                        the variation that are present in the original variables.
                    </p>
                    <p>
                        <img src="./Images/pca example.png" class="ImgCentreShort" alt="">
                    </p>
                    <p>
                        PCA is a decorrelation method (more on this below). PCA will linearly transform the data so that covarience values are all zeros, which in tern retains the components with the largets variences. While also getting rid of the components that have small
                        varience, therefore achieving dimensionality reduction. The Eigenvectors (more below) correspond to the different pricipal components.
                    </p>
                    <p>
                        <img src="./Images/pca reduction.png" class="ImgCentreShort" alt="">
                    </p>
                    <h2>Covarience Matrix</h2>
                    <p>
                        Slides 8 - 13
                    </p>
                    <p>
                        <img src="./Images/covarience example.png" class="ImgCentreShort" alt="">
                    </p>
                    <h2>Eigenvectors and Eigenvalues</h2>
                    <p>
                        Covariance matrix defines both the spread (variance), and the orientation (covariance) of the data. The vector that points into the direction of the largest spread of the data is the eigenvector with the largest eigenvalue. This eigenvalue equals the
                        spread (variance) in this direction (defined by the corresponding eigenvector). If the covariance matrix of our data is a diagonal matrix, such that the covariances are zero, then this means that the variances must be equal to
                        the eigenvalues <img src="https://latex.codecogs.com/gif.latex?\lambda" title="\lambda" />.
                    </p>
                    <p>
                        <img src="./Images/eigen vector and values.png" class="ImgCentreShort" alt="">
                    </p>
                    <p>
                        If the covariance matrix is not diagonal, such that the covariances are not zero. The eigenvalues still represent the variance magnitude in the direction of the largest spread of the data. The variance components of the covariance matrix still represent
                        the variance magnitude in the direction of the x-axis and y-axis. But since the data is not axis aligned, these values are not the same anymore.
                    </p>
                    <p>
                        <img src="./Images/eigen vec images 2.png" class="ImgCentreShort" alt="">
                    </p>
                </div>

                <div class="textBackground">
                    <h2>The Stages of PCA Dimensionality Reduction</h2>
                </div>

                <div class="textBackgroundLight">
                    <p>
                        There are three main steps to PCA dimensionality reduction. These are:
                        <ol>
                            <li>PCA will list the eigenvalues in descending order</li>
                            <li>It will then set a threshold and remove principal components that have small variances (small eigenvalues)</li>
                            <li>
                                The data is then projected back with reduced dimensionality.
                            </li>
                        </ol>



                    </p>
                    <p style="text-align: center;">
                        <img src="./Images/varience table.png" class="ImgCentreShort" alt=""> This table shows where the cut off area of PCs should take place.
                    </p>
                    <h2>SVD and PCA</h2>
                    <p>
                        For any Matrix <img src="https://latex.codecogs.com/gif.latex?X" title="X" />
                    </p>
                    <p>
                        <img src="./Images/USV.png" class="ImgCentreShort" alt="">
                    </p>
                    <p>
                        <ul>
                            <li>Data <img src="https://latex.codecogs.com/gif.latex?X" title="X" />, is one row per data point. The data is zero-centred.</li>
                            <li><img src="https://latex.codecogs.com/gif.latex?US" title="US" /> gives coordinates of rows of <img src="https://latex.codecogs.com/gif.latex?X" title="X" /> in the space of principle components.</li>
                            <li><img src="https://latex.codecogs.com/gif.latex?S" title="S" /> is diagagonal,
                                <img src="https://latex.codecogs.com/gif.latex?S_k&space;>&space;S_k&plus;1" title="S_k > S_k+1" /> is the
                                <img src="https://latex.codecogs.com/gif.latex?k^{th}" title="k^{th}" /> largest eigenvalue.</li>
                            <li>Rows of <img src="https://latex.codecogs.com/gif.latex?V^T" title="V^T" /> are the unit length of the eigenvectors.</li>
                        </ul>

                    </p>
                    <p>
                        PCA dimensionality reduction is setting “noise” to zero to achieve reduced dimensionality.
                    </p>
                    <p>
                        <img src="./Images/USV2.png" class="ImgCentreShort" alt="">
                    </p>

                </div>


                <!-- Footer -->
                <footer>

                    <!-- Copyright -->
                    <div>
                        <a href="dr.html"><i class="far fa-arrow-alt-circle-left"
                                style="padding-left: 38%;"></i>
                            Previous</a> <a href="lda.html"><span style="padding-left: 20px;">Next</span> <i
                                class="fas fa-arrow-circle-right"></i></a>
                    </div>
                    <!-- Copyright -->

                </footer>
                <!-- Footer -->

            </div>
        </main>
</body>

</html>